{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard","accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# 事前準備①：データセットの取得\n","データセットを本Colab上にダウンロードするには以下のどちらかを実行ください。"],"metadata":{"id":"f78kzzYlz6aY"}},{"cell_type":"markdown","source":["## [Option1] Google Driveから自身のPCに一度ダウンロードしてから、本Colabへアップロードする\n","Google DriveのURLはこちらです。\n","\n","https://drive.google.com/file/d/1naLdgbRmBq_QhosmXJkdmG4Pi9SiIyoo/view?usp=share_link"],"metadata":{"id":"e9qTo_56z-Md"}},{"cell_type":"markdown","source":["## [Option2] Google Driveから本Colabへ直接ダウンロードする\n","下記コマンドを実行する。\n","\n","（参考）https://qiita.com/namakemono/items/c963e75e0af3f7eed732"],"metadata":{"id":"KzVXlEc1z_Eg"}},{"cell_type":"code","source":["!curl -sc /tmp/cookie \"https://drive.google.com/uc?export=download&id=1naLdgbRmBq_QhosmXJkdmG4Pi9SiIyoo\" > /dev/null\n","!CODE=\"$(awk '/_warning_/ {print $NF}' /tmp/cookie)\"\n","!curl -Lb /tmp/cookie \"https://drive.google.com/uc?export=download&confirm=${CODE}&id=1naLdgbRmBq_QhosmXJkdmG4Pi9SiIyoo\" -o archive.zip\n","!unzip -q archive.zip"],"metadata":{"id":"atqTrYw5z9rI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---"],"metadata":{"id":"n5sipm2C0hLR"}},{"cell_type":"markdown","source":["# 事前準備②：学習済みモデルの取得\n","データセットを本Colab上にダウンロードするには以下のどちらかを実行ください。"],"metadata":{"id":"rqvarUs90ifz"}},{"cell_type":"markdown","source":["## [Option1] Google Driveから自身のPCに一度ダウンロードしてから、本Colabへアップロードする\n","Google DriveのURLはこちらです。\n","\n","https://drive.google.com/file/d/14PtYuFZc-5sB2n9lLUDku8bgyEKSLZG5/view?usp=share_link"],"metadata":{"id":"4vTki3fN0mK7"}},{"cell_type":"markdown","source":["## [Option2] Google Driveから本Colabへ直接ダウンロードする\n","下記コマンドを実行する。\n","\n","（参考）https://qiita.com/namakemono/items/c963e75e0af3f7eed732"],"metadata":{"id":"uqaD-xXq0rgZ"}},{"cell_type":"code","source":["!curl -sc /tmp/cookie \"https://drive.google.com/uc?export=download&id=14PtYuFZc-5sB2n9lLUDku8bgyEKSLZG5\" > /dev/null\n","!CODE=\"$(awk '/_warning_/ {print $NF}' /tmp/cookie)\"\n","!curl -Lb /tmp/cookie \"https://drive.google.com/uc?export=download&confirm=${CODE}&id=14PtYuFZc-5sB2n9lLUDku8bgyEKSLZG5\" -o drone_Trained.pth"],"metadata":{"id":"f_5tVDl50vBG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## インポートライブラリ"],"metadata":{"id":"3BAcb5S7VD9u"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"eYtwL15CAvv5"},"outputs":[],"source":["import numpy as np \n","import pandas as pd\n","\n","from sklearn.model_selection import train_test_split\n","\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms as T\n","import torchvision\n","import torch.nn.functional as F\n","from torch.autograd import Variable\n","\n","from PIL import Image\n","import cv2\n","import albumentations as A\n","\n","import statistics\n","\n","import time\n","import os\n","from tqdm.notebook import tqdm\n","\n","!pip install -q segmentation-models-pytorch\n","!pip install -q torchsummary\n","\n","from torchsummary import summary\n","import segmentation_models_pytorch as smp\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","if torch.cuda.is_available():\n","  print(torch.cuda.get_device_name())"]},{"cell_type":"markdown","source":["## パス設定\n","画像、モデル、出力画像の保存先のパス指定"],"metadata":{"id":"s2_N848nv2wb"}},{"cell_type":"code","source":["IMAGE_PATH = #ドローン画像が格納されているフォルダのパス（相対パスでも絶対パスでもOK）\n","MASK_PATH = #ドローン画像のマスク画像（正解ラベル画像）が格納されているフォルダのパス（相対パスでも絶対パスでもOK）\n","MODEL_PATH = #学習済みモデルファイルへのパス（相対パスでも絶対パスでもOK）\n","\n","# 作成されたモデルファイルをを格納するフォルダを作成およびパスを変数にセット\n","!mkdir -p Custum_Model\n","SAVE_PATH = 'Custum_Model'"],"metadata":{"id":"Zr-FYalgCec3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## データセットの構築\n","画像ファイル名のリストを作成します。"],"metadata":{"id":"872VOvhlv9Zx"}},{"cell_type":"code","source":["def create_df():\n","    name = []\n","    for dirname, _, filenames in os.walk(IMAGE_PATH):\n","        for filename in filenames:\n","            name.append(filename.split('.')[0])\n","    \n","    return pd.DataFrame({'id': name}, index = np.arange(0, len(name)))\n","\n","df = create_df()\n","print('全画像数: ', len(df))"],"metadata":{"id":"g9vlF17-DHrP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X_trainval, X_test = train_test_split(df['id'].values, test_size=0.1, random_state=19)#全データの10%をテストデータとしてランダムに分離\n","X_train, X_val = train_test_split(X_trainval, test_size=0.15, random_state=19)#全データの13.5%を検証データ,残りを訓練データとしてランダムに分離\n","\n","print('Train Size   : ', len(X_train))\n","print('Val Size     : ', len(X_val))\n","print('Test Size    : ', len(X_test))"],"metadata":{"id":"lzvdpNlWDOqN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## アノテーションの色の定義"],"metadata":{"id":"3Vsv6NvdWmwL"}},{"cell_type":"code","source":["mapping = {(0, 0, 0): 0,\n","           (150, 143, 9): 1,\n","            }        "],"metadata":{"id":"MSO_hcvvGEZe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 3次元アノテーションデータを2次元データに変換(mappingに定義されたRGB値からvalue値に変換)"],"metadata":{"id":"JR0wUlxqXTkm"}},{"cell_type":"code","source":["def convert_rgb_to_value(target):\n","  h = target.shape[0]#画像の高さの取得\n","  w = target.shape[1]#画像の横幅の取得\n","  target = target.permute(2,0,1).contiguous()#テンソルの形を変換(H,W,C)->(C,H,W)\n","  mask = torch.empty(h, w, dtype=torch.long)#(H,W)の2次元型を用意\n","  \n","  for k in mapping:#マップで定義したクラスの数繰り返し検出する\n","    idx = (target==torch.tensor(k, dtype=torch.uint8).unsqueeze(1).unsqueeze(2))#targetのある画素がmappingに定義し現在対象となっている値Kと一致すればTrueを返すテンソルidxを生成(C,H,W) \n","                                                                                #(RGB値しか持たないtorch.tensor(k, dtype=torch.uint8)に対してunsueezeを2回行うことで比較可能な3次元形式に変形している)\n","    validx = (idx.sum(0) == 3)#validx:RGB値すべてが一致した場合True,一致しない場合Falseの2次元テンソル（H,W）\n","    mask[validx] = torch.tensor(mapping[k], dtype=torch.long)#validxがTrueだった場所に現在のKのRGB値を持つvalue値をmaskに代入\n","  \n","  return mask"],"metadata":{"id":"JlWg6PhVGbB6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 学習時のデータセットの定義\n"],"metadata":{"id":"EfNstf3uXbsE"}},{"cell_type":"code","source":["class DroneDataset(Dataset):\n","    \n","    def __init__(self, img_path, mask_path, X, mean, std, transform=None, patch=False):\n","        self.img_path = img_path\n","        self.mask_path = mask_path\n","        self.X = X\n","        self.transform = transform\n","        self.patches = patch\n","        self.mean = mean\n","        self.std = std\n","\n","\n","    def __len__(self):\n","        return len(self.X)\n","    \n","    def __getitem__(self, idx):\n","        img = cv2.imread(os.path.join(self.img_path, self.X[idx] + '.jpg'))\n","        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","        mask = cv2.imread(os.path.join(self.mask_path, self.X[idx] + '.png'))\n","        filename = str(self.X[idx] + '.jpg')\n","        \n","        if self.transform is not None:\n","            aug = self.transform(image=img, mask=mask)\n","            img = Image.fromarray(aug['image'])\n","            mask = aug['mask']\n","        \n","        if self.transform is None:\n","            img = Image.fromarray(img)\n","        \n","        t = T.Compose([T.ToTensor(), T.Normalize(self.mean, self.std)])\n","        img = t(img)\n","        mask = torch.from_numpy(mask).long()\n","        \n","        if self.patches:\n","            img, mask = self.tiles(img, mask)\n","\n","        mask = convert_rgb_to_value(mask)        \n"," \n","        return img, mask, filename\n","    \n","    def tiles(self, img, mask):\n","\n","        img_patches = img.unfold(1, 512, 512).unfold(2, 768, 768) \n","        img_patches  = img_patches.contiguous().view(3,-1, 512, 768) \n","        img_patches = img_patches.permute(1,0,2,3)\n","        \n","        mask_patches = mask.unfold(0, 512, 512).unfold(1, 768, 768)\n","        mask_patches = mask_patches.contiguous().view(-1, 512, 768)\n","        \n","        return img_patches, mask_patches"],"metadata":{"id":"fWwUMiblMQUR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## データセットの変形(Data Augmentation)\n"],"metadata":{"id":"ZAeQQsMStRfa"}},{"cell_type":"code","source":["mean=[0.485, 0.456, 0.406]\n","std=[0.229, 0.224, 0.225]\n","\n","t_train = A.Compose([\n","    A.Resize(704, 1056, interpolation=cv2.INTER_NEAREST), \n","    A.HorizontalFlip(), \n","    A.VerticalFlip(), \n","    A.GridDistortion(p=0.2), \n","    A.RandomBrightnessContrast((0,0.5),(0,0.5)),\n","    A.GaussNoise()])\n","\n","t_val = A.Compose([\n","    A.Resize(704, 1056, interpolation=cv2.INTER_NEAREST), \n","    A.HorizontalFlip(),\n","    A.GridDistortion(p=0.2)])\n","\n","#datasets\n","train_set = DroneDataset(IMAGE_PATH, MASK_PATH, X_train, mean, std, t_train, patch=False)\n","val_set = DroneDataset(IMAGE_PATH, MASK_PATH, X_val, mean, std, t_val, patch=False)"],"metadata":{"id":"uMGoHWibMXlc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# データとモデルのロード"],"metadata":{"id":"LflCyxAuwWkc"}},{"cell_type":"markdown","source":["## 訓練データと検証データのロード"],"metadata":{"id":"bt85lKoHs50q"}},{"cell_type":"code","source":["batch_size= 4 \n","\n","train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n","val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=True)    "],"metadata":{"id":"ifZsuTIUxvCp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## モデルのロード＆事前学習済みのパラメータのロード\n","【注意】今回認識したいのは「車」「それ以外」という2クラスだけだが、ベースの学習済みモデルが24クラスを分類するように学習されているため、合わせる意味でクラス数を24としている。"],"metadata":{"id":"eTDbIDodXnoM"}},{"cell_type":"code","source":["model = smp.Unet('mobilenet_v2', encoder_weights='imagenet', classes=24, activation=None, encoder_depth=5, decoder_channels=[256, 128, 64, 32, 16])\n","model.load_state_dict(torch.load(MODEL_PATH))\n","\n","# 今回の学習済みモデルを使用しない場合は下記のようにモデルを定義する。クラスが2つあることに注意。「車」と「それ以外」。\n","#model = smp.Unet('mobilenet_v2', encoder_weights='imagenet', classes=2, activation=None, encoder_depth=5, decoder_channels=[256, 128, 64, 32, 16])"],"metadata":{"id":"eQsxptkWH7IQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 学習設定"],"metadata":{"id":"GTup00-Owd_H"}},{"cell_type":"markdown","source":["## 検証時の精度評価関数の定義"],"metadata":{"id":"b7b69XvKtpih"}},{"cell_type":"code","source":["def pixel_accuracy(output, mask):\n","    with torch.no_grad():\n","        output = torch.argmax(F.softmax(output, dim=1), dim=1)\n","        correct = torch.eq(output, mask).int()\n","        accuracy = float(correct.sum()) / float(correct.numel())\n","    return accuracy\n","\n","def mIoU(pred_mask, mask, smooth=1e-10, n_classes=23):\n","    with torch.no_grad():\n","        pred_mask = F.softmax(pred_mask, dim=1)\n","        pred_mask = torch.argmax(pred_mask, dim=1)\n","        pred_mask = pred_mask.contiguous().view(-1)\n","        mask = mask.contiguous().view(-1)\n","\n","        iou_per_class = []\n","        for clas in range(0, n_classes): #loop per pixel class\n","            true_class = pred_mask == clas\n","            true_label = mask == clas\n","\n","            if true_label.long().sum().item() == 0: #no exist label in this loop\n","                iou_per_class.append(np.nan)\n","            else:\n","                intersect = torch.logical_and(true_class, true_label).sum().float().item()\n","                union = torch.logical_or(true_class, true_label).sum().float().item()\n","\n","                iou = (intersect + smooth) / (union +smooth)\n","                iou_per_class.append(iou)\n","        return np.nanmean(iou_per_class)"],"metadata":{"id":"hOk0iz2vteQD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 学習率の計算"],"metadata":{"id":"OaF73augt5pn"}},{"cell_type":"code","source":["def get_lr(optimizer):\n","    for param_group in optimizer.param_groups:\n","        return param_group['lr']"],"metadata":{"id":"FSfqQXyrtxBq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## モデルの学習＆検証"],"metadata":{"id":"kAbmM70kuJjC"}},{"cell_type":"code","source":["def fit(epochs, model, train_loader, val_loader, criterion, optimizer, scheduler, patch=False):\n","    torch.cuda.empty_cache()\n","    train_losses = []\n","    test_losses = []\n","    val_iou = []; val_acc = []\n","    train_iou = []; train_acc = []\n","    lrs = []\n","    min_loss = np.inf\n","    decrease = 1 ; not_improve=0\n","\n","    model.to(device)\n","    fit_time = time.time()\n","    for e in range(epochs):\n","        since = time.time()\n","        running_loss = 0\n","        iou_score = 0\n","        accuracy = 0\n","        #training loop\n","        model.train()\n","        \n","        for i, data in enumerate(tqdm(train_loader)):\n","            #training phase\n","            image_tiles, mask_tiles , filename = data\n","\n","            if patch:\n","                bs, n_tiles, c, h, w = image_tiles.size()\n","\n","                image_tiles = image_tiles.view(-1,c, h, w)\n","                mask_tiles = mask_tiles.view(-1, h, w)\n","            \n","            image = image_tiles.to(device); mask = mask_tiles.to(device);\n","\n","            #forward\n","            output = model(image)\n","            loss = criterion(output, mask)\n","\n","            #evaluation metrics\n","            iou_score += mIoU(output, mask)\n","            accuracy += pixel_accuracy(output, mask)\n","\n","            #backward\n","            loss.backward()\n","            optimizer.step() #update weight img = Image.fromarray(img)\n","            lrs.append(get_lr(optimizer))\n","            scheduler.step() \n","            \n","            running_loss += loss.item()\n","            \n","        else:\n","            model.eval()\n","            test_loss = 0\n","            test_accuracy = 0\n","            val_iou_score = 0\n","            #validation loop\n","            with torch.no_grad():\n","                for i, data in enumerate(tqdm(val_loader)):\n","                    #reshape to 9 patches from single image, delete batch size\n","                    image_tiles, mask_tiles ,filename = data\n","\n","                    if patch:\n","                        bs, n_tiles, c, h, w = image_tiles.size()\n","\n","                        image_tiles = image_tiles.view(-1,c, h, w)\n","                        mask_tiles = mask_tiles.view(-1, h, w)\n","                    \n","                    image = image_tiles.to(device); mask = mask_tiles.to(device);\n","                    output = model(image)\n","                    #evaluation metrics\n","                    val_iou_score +=  mIoU(output, mask)\n","                    test_accuracy += pixel_accuracy(output, mask)\n","                    #loss\n","                    loss = criterion(output, mask)                                  \n","                    test_loss += loss.item()\n","            \n","            #calculatio mean for each batch\n","            train_losses.append(running_loss/len(train_loader))\n","            test_losses.append(test_loss/len(val_loader))\n","\n","\n","            if min_loss > (test_loss/len(val_loader)):\n","                print('Loss Decreasing.. {:.3f} >> {:.3f} '.format(min_loss, (test_loss/len(val_loader))))\n","                min_loss = (test_loss/len(val_loader))\n","                decrease += 1\n","                if decrease % 5 == 0:\n","                    print('saving model...')\n","                    torch.save(model.state_dict(), SAVE_PATH + '/Unet-Mobilenet_v2_mIoU-{:.3f}.pth'.format(val_iou_score/len(val_loader)))\n","                    \n","\n","            if (test_loss/len(val_loader)) > min_loss:\n","                not_improve += 1\n","                min_loss = (test_loss/len(val_loader))\n","                print(f'Loss Not Decrease for {not_improve} time')\n","                #if not_improve == 7:\n","                    #print('Loss not decrease for 7 times, Stop Training')\n","                    #break\n","            \n","            #iou\n","            val_iou.append(val_iou_score/len(val_loader))\n","            train_iou.append(iou_score/len(train_loader))\n","            train_acc.append(accuracy/len(train_loader))\n","            val_acc.append(test_accuracy/ len(val_loader))\n","            print(\"Epoch:{}/{}..\".format(e+1, epochs),\n","                  \"Train Loss: {:.3f}..\".format(running_loss/len(train_loader)),\n","                  \"Val Loss: {:.3f}..\".format(test_loss/len(val_loader)),\n","                  \"Train mIoU:{:.3f}..\".format(iou_score/len(train_loader)),\n","                  \"Val mIoU: {:.3f}..\".format(val_iou_score/len(val_loader)),\n","                  \"Train Acc:{:.3f}..\".format(accuracy/len(train_loader)),\n","                  \"Val Acc:{:.3f}..\".format(test_accuracy/len(val_loader)),\n","                  \"Time: {:.2f}m\".format((time.time()-since)/60))\n","        \n","    history = {'train_loss' : train_losses, 'val_loss': test_losses,\n","               'train_miou' :train_iou, 'val_miou':val_iou,\n","               'train_acc' :train_acc, 'val_acc':val_acc,\n","               'lrs': lrs}\n","    print('Total time: {:.2f} m' .format((time.time()- fit_time)/60))\n","    return history"],"metadata":{"id":"94IHIBIvuCC7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## ハイパーパラメータ定義"],"metadata":{"id":"xS9aL-7Eusi-"}},{"cell_type":"code","source":["max_lr = 1e-3\n","epoch = 1\n","weight_decay = 1e-4\n","\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.AdamW(model.parameters(), lr=max_lr, weight_decay=weight_decay)\n","sched = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr, epochs=epoch,\n","                                            steps_per_epoch=len(train_loader))"],"metadata":{"id":"gEiS-V5vuoR2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 学習の実行とパラメータの保存"],"metadata":{"id":"W5gOk8zavNb9"}},{"cell_type":"code","source":["history = fit(epoch, model, train_loader, val_loader, criterion, optimizer, sched)\n","\n","torch.save(model.state_dict(), SAVE_PATH + '/drone_Custum_Trained.pth')"],"metadata":{"id":"NQOJ55zxvBDu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","## 【注意】このセルは本講義のための特別処理です。\n","本来はモデルの学習を数百エポック実行したいのですが、時間の都合上困難なため、すでに数百エポック分学習したモデルにすり替えてこの後の処理を進めていきます。"],"metadata":{"id":"LwG_dLd3C3ut"}},{"cell_type":"code","source":["!curl -sc /tmp/cookie \"https://drive.google.com/uc?export=download&id=1JXPHg4brau1T93z79VNr4VqLeCEx2CcW\" > /dev/null\n","!CODE=\"$(awk '/_warning_/ {print $NF}' /tmp/cookie)\"\n","!curl -Lb /tmp/cookie \"https://drive.google.com/uc?export=download&confirm=${CODE}&id=1JXPHg4brau1T93z79VNr4VqLeCEx2CcW\" -o drone_Custum_Trained.pth\n","model.load_state_dict(torch.load('drone_Custum_Trained.pth'))"],"metadata":{"id":"C5tzjgZbCaf1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---"],"metadata":{"id":"jDeAvbp3Ddg2"}},{"cell_type":"markdown","source":["## テスト画像を使用してモデルの精度を測る"],"metadata":{"id":"ha8gGHiq3CwV"}},{"cell_type":"code","source":["reverse_mapping = {1: (150, 143, 9),\n","                   0: (0, 0, 0),\n","}      \n","\n","def visualize(temp):\n","    r = temp.copy()\n","    g = temp.copy()\n","    b = temp.copy()\n","    for l in range(0,len(reverse_mapping)):\n","        r[temp==l]=reverse_mapping[l][0]\n","        g[temp==l]=reverse_mapping[l][1]\n","        b[temp==l]=reverse_mapping[l][2]\n","\n","    rgb = np.zeros((temp.shape[1], temp.shape[2],3))\n","\n","    rgb[:,:,0] = (r)\n","    rgb[:,:,1] = (g)\n","    rgb[:,:,2] = (b)\n","    return rgb"],"metadata":{"id":"ulXGHNohxCXy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class DroneTestDataset(Dataset):\n","    \n","    def __init__(self, img_path, mask_path, X, transform=None):\n","        self.img_path = img_path\n","        self.mask_path = mask_path\n","        self.X = X\n","        self.transform = transform\n","      \n","    def __len__(self):\n","        return len(self.X)\n","    \n","    def __getitem__(self, idx):\n","        img = cv2.imread(os.path.join(self.img_path, self.X[idx] + '.jpg'))\n","        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","        mask = cv2.imread(os.path.join(self.mask_path, self.X[idx] + '.png'))\n","        filename = str(self.X[idx])\n","        \n","        if self.transform is not None:\n","            aug = self.transform(image=img, mask=mask)\n","            img = Image.fromarray(aug['image'])\n","            mask = aug['mask']\n","        \n","        if self.transform is None:\n","            img = Image.fromarray(img)\n","        \n","        mask = torch.from_numpy(mask).long()\n","\n","        mask = convert_rgb_to_value(mask)\n","        \n","        return img, mask, filename"],"metadata":{"id":"1zLsqxz973Fe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["t_test = A.Resize(704, 1056, interpolation=cv2.INTER_NEAREST)\n","test_set = DroneTestDataset(IMAGE_PATH, MASK_PATH, X_test, transform=t_test)"],"metadata":{"id":"r66vSBV87tYZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def predict_image_mask_miou(model, image, mask, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]):\n","    model.eval()\n","    t = T.Compose([T.ToTensor(), T.Normalize(mean, std)])\n","    image = t(image)\n","    model.to(device); image=image.to(device)\n","    mask = mask.to(device)\n","    with torch.no_grad():\n","        \n","        image = image.unsqueeze(0)\n","        mask = mask.unsqueeze(0)\n","        \n","        output = model(image)\n","        score = mIoU(output, mask)\n","        masked = torch.argmax(output, dim=1)\n","        masked = masked.cpu().squeeze(0)\n","    return masked, score\n","\n","def predict_image_mask_pixel(model, image, mask, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]):\n","    model.eval()\n","    t = T.Compose([T.ToTensor(), T.Normalize(mean, std)])\n","    image = t(image)\n","    model.to(device); image=image.to(device)\n","    mask = mask.to(device)\n","    with torch.no_grad():\n","        \n","        image = image.unsqueeze(0)\n","        mask = mask.unsqueeze(0)\n","        \n","        output = model(image)\n","        acc = pixel_accuracy(output, mask)\n","        masked = torch.argmax(output, dim=1)\n","        masked = masked.cpu().squeeze(0)\n","    return masked, acc"],"metadata":{"id":"xF5jzTtg3Qx5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def miou_score(model, test_set):\n","    score_iou = []\n","    for i in tqdm(range(len(test_set))):\n","        img, mask ,filename= test_set[i]\n","        pred_mask, score = predict_image_mask_miou(model, img, mask)\n","        pred_mask = pred_mask.cpu().numpy().copy()\n","        pred_mask = pred_mask.reshape(1,pred_mask.shape[0],pred_mask.shape[1])\n","        cv2.imwrite(os.path.join(SAVE_PATH,filename + \".png\"), visualize(pred_mask),[cv2.IMWRITE_PNG_COMPRESSION,9])#ここで画像を保存\n","        score_iou.append(score)\n","    mean_iou = statistics.mean(score_iou)\n","    return mean_iou\n","\n","def pixel_acc(model, test_set):\n","    accuracy = []\n","    for i in tqdm(range(len(test_set))):\n","        img, mask,filename = test_set[i]\n","        pred_mask, acc = predict_image_mask_pixel(model, img, mask)\n","        accuracy.append(acc)\n","    mean_acc = statistics.mean(accuracy)\n","    return mean_acc"],"metadata":{"id":"7UWWQgOn3Y3h"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["mob_miou = miou_score(model, test_set)\n","print(\"miou: \" + str(mob_miou))\n","mob_acc = pixel_acc(model, test_set)\n","print(\"acc: \" + str(mob_acc))"],"metadata":{"id":"tBKhXd3J3fkG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 画像を一枚だけランダムに推論する"],"metadata":{"id":"PRQRa0jvtqub"}},{"cell_type":"code","source":["def mIoU(pred_mask, mask, smooth=1e-10, n_classes=23):\n","    with torch.no_grad():\n","        pred_mask = F.softmax(pred_mask, dim=1)\n","        pred_mask = torch.argmax(pred_mask, dim=1)\n","        pred_mask = pred_mask.contiguous().view(-1)\n","        mask = mask.contiguous().view(-1)\n","\n","        iou_per_class = []\n","        for clas in range(0, n_classes): #loop per pixel class\n","            true_class = pred_mask == clas\n","            true_label = mask == clas\n","\n","            if true_label.long().sum().item() == 0: #no exist label in this loop\n","                iou_per_class.append(np.nan)\n","            else:\n","                intersect = torch.logical_and(true_class, true_label).sum().float().item()\n","                union = torch.logical_or(true_class, true_label).sum().float().item()\n","\n","                iou = (intersect + smooth) / (union +smooth)\n","                iou_per_class.append(iou)\n","        return np.nanmean(iou_per_class)"],"metadata":{"id":"eYDV_xiuwr3V"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def predict_image_mask_miou(model, image, mask, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]):\n","    model.eval()\n","    t = T.Compose([T.ToTensor(), T.Normalize(mean, std)])\n","    image = t(image)\n","    model.to(device); image=image.to(device)\n","    mask = mask.to(device)\n","    with torch.no_grad():\n","        \n","        image = image.unsqueeze(0)\n","        mask = mask.unsqueeze(0)\n","        \n","        output = model(image)\n","        score = mIoU(output, mask)\n","        masked = torch.argmax(output, dim=1)\n","        masked = masked.cpu().squeeze(0)\n","    return masked, score"],"metadata":{"id":"gpPndn6IwrQv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import random\n","\n","t_test = A.Resize(704, 1056, interpolation=cv2.INTER_NEAREST)\n","test_set = DroneTestDataset(IMAGE_PATH, MASK_PATH, X_test, transform=t_test)\n","\n","index = random.randint(0, len(test_set))\n","img, mask, filename = test_set[index]\n","pred_mask, score = predict_image_mask_miou(model, img, mask)\n","pred_mask = pred_mask.cpu().numpy().copy()\n","pred_mask = pred_mask.reshape(1,pred_mask.shape[0],pred_mask.shape[1])\n","print(filename)\n","\n","\n","import matplotlib.pyplot as plt\n","plt.figure(figsize=(20,16))\n","plt.subplot(1,2,1)\n","plt.imshow(img)\n","plt.subplot(1,2,2)\n","plt.imshow(visualize(pred_mask))"],"metadata":{"id":"dinXo3fDtrh_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"BskP7gkyBcZo"},"execution_count":null,"outputs":[]}]}